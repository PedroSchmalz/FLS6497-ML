---
title: "Exercício Aula 9"
author: "Pedro Schmalz"
date: "2022-11-23"
output:
  pdf_document: default
  html_document: default
---

- Capítulo 9 livro Python sobre Kmeans, aprendizado semi-supervisionado (pode ser útil para TTs)

```{r, echo=FALSE}
knitr::opts_chunk$set(echo=F, error=FALSE, warning=FALSE, message=FALSE)
```

```{r}
#Pacotes utilizados

if (!require("pacman")) install.packages("pacman"); # O Pacote "pacman" permite carregar os pacotes com menos código

# Carregando os pacotes

pacman::p_load("tidyverse",  "dplyr", "datasets", "ggplot2", "readxl", "haven", "knitr", "reshape2", "broom", "modelr", "stargazer", "jtools", "purrr", "mlr3", "mlr3measures", "mlr3viz", "mlr3learners", "mlr3extralearners", "mlr3tuning", "GGally", "kknn", "glmnet", "quanteda", "janitor", "ranger", "mlr3verse", "igraph", "earth", "randomForest", "xgboost", "gbm", 'kernlab', 'mlr3cluster', 'factoextra', 'dbscan', 'vars')

#remotes::install_github("mlr-org/mlr3forecasting") #pacote em desenvolvimento
#install.packages("vars")

#devtools::install_github("mlr-org/mlr3extralearners", force = TRUE)
library(mlr3extralearners)
```

# Exercícios 9

-> trade off valor de K, bias vs. efficiency
-> -50 live one out
-> K padrão na maioria dos frameworks =5 ou =3
-> K grande -> repeated cross validation
-> até agora usavamos holdout, mas é xexelento
-> validação por bootstrap é pouco eficiente e não é usada na prática
-> levar em conta estrutura de cluster na hora de fazer cross-validation
-> estratos:

obs não necessariamente correlacionadas
base de treino precisa contemplar todos os estratos 
estratificar por município e tals

->mlr3 é fraco de validação com série temporal
janela fixa vs. janela móvel


# 1) Cross-validation
Para esse exercício, usaremos uma base de dados das candidaturas à Câmara dos Deputados em 2014 que contém, entre outros, variáveis como o sexo, a raça, a escolaridade e o status de reeleição das candidaturas, bem como uma dummy (resultado) que indica se a candidatura foi () ou não () eleita (Machado, Campos, e Recch 2020).


```{r}

link <- "https://raw.githubusercontent.com/FLS-6497/datasets/main/aula9/camara_2014.csv"
dados <- readr::read_csv2(link) %>%
  mutate_if(is.character, as.factor)
```

# a) Básico
Crie uma pipeline para estandardizar variáveis numéricas (ou transformar variáveis categóricas em dummies) com algum modelo de classificação da sua escolha e o valide usando K-fold com K=5 e, depois, com K=10 .

```{r}

# Define a task
tsk <- as_task_classif(resultado ~ ., data = dados)

# Cria uma pipeline
gr <- po("scale") %>>%
  po("learner", learner = lrn("classif.naive_bayes")) %>%
  as_learner()

# K-fold
design <- benchmark_grid(
  tasks = tsk,
  learners = list(gr),
  resamplings = rsmp("cv", folds = 5)
)

resultados <- benchmark(design)
resultados$score(msrs(c("classif.fbeta", "classif.precision", "classif.recall")))

# K-fold, K=10
design <- benchmark_grid(
  tasks = tsk,
  learners = list(gr),
  resamplings = rsmp("cv", folds = 10)
)

resultados <- benchmark(design)
resultados$score(msrs(c("classif.fbeta", "classif.precision", "classif.recall")))


```

# b) LOO

Sorteie apenas algumas observações do banco completo (50, por exemplo) e, em vez de usar K-fold, desta vez use LOO como estratégia de validação (no mlr3, a função chama-se loo; no sklearn, LeaveOneOut).


```{r}
# Estrato

# amostra menor pro loo loo

dados_menor <- sample_n(dados, 100)


tsk <- as_task_classif(resultado ~ ., stratum = "resultado", data = dados_menor)

gr <- po("scale") %>>% 
  po("encode") #Fazer uns trens de dummy
   po("learner", learner = lrn("classif.naive_bayes")) %>%
  as_learner()

resultados <- benchmark(design)
resultados$aggregate(msr("classif.ce"))

x <-gr$train(tsk)
x$encode.output$data() %>%
  view()

```

# c) Mantendo balanço

Na base de dados, há muito menos candidaturas eleitas do que não-eleitas. Para evitar que amostras de treino e de teste percam esse balanço original, use K-fold estratificado (no mlr3, basta declarar stratum = variavel na task; no sklearn, use StratifiedKFold).

```{r}

#

tsk <- as_task_classif(resultado ~ ., stratum = "resultado", data = dados)

gr1 <- po("scale") %>>% #Fazer uns trens de dummy
  po("encode")  %>>%
  po("learner", learner = lrn("classif.naive_bayes", predict_type = "prob")) %>%
  as_learner()
  
gr2 <- po("scale") %>>% #Fazer uns trens de dummy
  po("encode") %>>%
  po("learner", learner = lrn("classif.svm", predict_type = "prob")) %>%
  as_learner()
  
design <- benchmark_grid(
  tasks = tsk,
  learners = list(gr1, gr2),
  resamplings = rsmp("cv", folds = 10)
  )

resultados <- benchmark(design)
resultados$aggregate(msrs(c("classif.auc", "classif.prauc")))
autoplot(resultados, type = 'prc')




```

# d) Repetindo o processo
Finalmente, use repeated k-fold para minimizar a variação decorrente do sorteio no particionamento das amostras (no mlr3, com repeated_cv; no sklearn, com RepeatedKFold ou com RepeatedStratifiedKFold).

```{r}
# Repeated K-fold
design <- benchmark_grid(
  tasks = tsk,
  learners = list(gr1, gr2),
  resamplings = rsmp("repeated_cv", folds = 10, repeats = 10)
)


resultados <- benchmark(design)
resultados$aggregate(msrs(c("classif.auc", "classif.prauc")))
autoplot(resultados, type = 'prc')

```

# Workflow de validação
Para este exercício, precisaremos separar a nossa amostra de uma forma mais próxima daquela usada em projetos reais: treino, teste e validação. Para tanto:

a) Holdout
Faça um holdout inicial da base, separando 90% dela para treino e teste e 10% para validação.

b) Cross-validation
Com os 90% restanted da base, treine e valide um modelo usando alguma estratégia de cross-validation. Ao final, quando encontrar o melhor modelo, treine ele em todos os 90% das observações e o valide na base de validação com 10% de observações.

```{r}

link <- "https://raw.githubusercontent.com/FLS-6497/datasets/main/aula9/camara_2014.csv"
dados <- readr::read_csv2(link) %>%
  mutate_if(is.character, as.factor) %>% 
  mutate(id =1:n())

validacao <- sample_frac(dados, 0.1)

dados <- dados %>%
  filter(!id %in% validacao$id)

tsk <- as_task_classif(resultado ~ ., stratum = "resultado", data = dados)




gr1 <- po("scale") %>>% #Fazer uns trens de dummy
  po("encode")  %>>%
  po("learner", learner = lrn("classif.ranger", predict_type = "prob")) %>%
  as_learner()
  
gr2 <- po("scale") %>>% #Fazer uns trens de dummy
  po("encode") %>>%
  po("learner", learner = lrn("classif.svm", predict_type = "prob")) %>%
  as_learner()
  
design <- benchmark_grid(
  tasks = tsk,
  learners = list(gr1, gr2),
  resamplings = rsmp("cv", folds = 10)
  )

resultados <- benchmark(design)
resultados$aggregate(msrs(c("classif.auc", "classif.prauc")))
autoplot(resultados, type = 'prc')

```

```{r}
tsk <- as_task_classif(resultado ~ ., stratum = "resultado", data = dados)
modelo <- gr1$train(tsk)

validacao <- validacao %>% mutate(resultado = as.factor(resultado))

pred <- modelo$predict_newdata(validacao)
validacao$pred <- pred$response

```


```{r}

pred$confusion
```
```{r}
autoplot(pred)
```
```{r}
pred$score(msrs(c("classif.acc", "classif.bacc", "classif.ce", "classif.mbrier")))
```


# Usando mais dados

Neste exercício, vamos voltar à base de dados climático de São Bernardo do Campo e, com o que aprendemos nas últimas aulas, vamos tentar melhorar nosso desempenho na tarefa de predizer temperatura máxima diária. Carregue a base com:


```{r}
link <- "https://raw.githubusercontent.com/jacobwright32/Web_Scraper_AI_Core_Project/bb4865ae568e23ab8fadb6ea58cf117df2164ef3/web%20scraping/Cleaned%20Data/Brazil_Sao%20Bernardo%20Do%20Campo_Cleaned.csv"


dados <- readr::read_csv(link) %>% 
  mutate_if(is.character, as.factor) %>% 
  mutate(id =1:n()) %>% 
  mutate(lag_max_temp = lag(maximum_temprature)) %>%  #lag antes da base de validação
  dplyr::select(-c('date', 'country', 'city', 'wind_direction')) %>% 
  na.omit()
  


validacao <- sample_frac(dados, 0.1)

dados <- dados %>%
  filter(!id %in% validacao$id) %>% 
  dplyr::select(-id)

tsk <- as_task_regr(maximum_temprature ~., data = dados)

```

a) Novo Workflow

Monte um workflow para melhorar o desempenho na tarefa de predizer maximum_temprature. Em particular, considere o seguinte:

Pré-processar variáveis contínuas (minmax ou estandardização);
Reduzir dimensionalidade (PCA ou kernelpca);
Considerar combinações não-lineares (criando polinômios ou usando MARS)
Usar ensemble, inclusive com stacking
Usar uma estratégia de validação que deixe mais dados para treino (K-fold com um  ou )
Considerar a estrutura temporal dos dados (é possível criar uma variável lag de maximum_temprature, o transformar o problema em um de série temporal e usar walk-forward validation)


```{r}
# KNN

gr2knn <- po("mutate") %>>% #não tem função automatica dentro do mlr3, preciso criar polinômios 'manualmente'
  po("scale") %>>% 
  po("learner", learner = lrn("regr.kknn")) %>%
  as_learner()

gr2knn$param_set$values$mutate.mutation <- list(
  teste = ~ cloud_coverage^2 + cloud_coverage^3,
  teste1 = ~ cloud_coverage^3,
  teste2 = ~ pressure^2 + pressure^3) #nome da variável criada = ~variáveis que vou utilizar
  teste3 = ~ pressure^3
```


```{r}
# Earth
  
gr2earth <- po("mutate") %>>% #não tem função automatica dentro do mlr3, preciso criar polinômios 'manualmente'
  po("scale") %>>% 
  po("learner", learner = lrn("regr.earth")) %>%
  as_learner()

gr2earth$param_set$values$mutate.mutation <- list(
  teste = ~ cloud_coverage^2 + cloud_coverage^3,
  teste1 = ~ cloud_coverage^3,
  teste2 = ~ pressure^2 + pressure^3) #nome da variável criada = ~variáveis que vou utilizar
  teste3 = ~ pressure^3
```


```{r}
# Random Forest
  
gr2tree <- po("mutate") %>>% #não tem função automatica dentro do mlr3, preciso criar polinômios 'manualmente'
  po("scale") %>>% 
  po("learner", learner = lrn("regr.ranger")) %>%
  as_learner()

gr2tree$param_set$values$mutate.mutation <- list(
  teste = ~ cloud_coverage^2 + cloud_coverage^3,
  teste1 = ~ cloud_coverage^3,
  teste2 = ~ pressure^2 + pressure^3) #nome da variável criada = ~variáveis que vou utilizar
  teste3 = ~ pressure^3
```

```{r}
# SVM

gr2svm <- po("mutate") %>>% #não tem função automatica dentro do mlr3, preciso criar polinômios 'manualmente'
  po("scale") %>>% 
  po("learner", learner = lrn("regr.svm")) %>%
  as_learner()

gr2svm$param_set$values$mutate.mutation <- list(
  teste = ~ cloud_coverage^2 + cloud_coverage^3,
  teste1 = ~ cloud_coverage^3,
  teste2 = ~ pressure^2 + pressure^3) #nome da variável criada = ~variáveis que vou utilizar
  teste3 = ~ pressure^3
```



```{r}
design <- benchmark_grid(
  tasks = tsk,
  learners = list(gr2knn, gr2earth, gr2tree, gr2svm),
  resamplings = rsmp("cv", folds = 10)
  )



resultados <- benchmark(design)
resultados$aggregate(msr("regr.rmse"))
```


