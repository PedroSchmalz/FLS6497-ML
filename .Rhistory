tks_disc <- corpus_disc %>%
tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(pattern = stopwords("pt")) %>%
tokens_remove(min_nchar = 5)
# 3) Matriz bag-of-words
tks_dfm <- dfm(tks_disc) %>%
dfm_trim(min_docfreq = 0.05, docfreq_type = "prop")
# 4) Transforma em tibble e inclui o target
dados <- as.matrix(tks_dfm) %>%
as_tibble() %>%
janitor::clean_names()
dados$y <- discursos$presidente
}
saco(discursos)
saco <- function(discursos){
# 1) Criar um corpus
corpus_disc <- corpus(discursos, text_field = discursos$discurso)
# 2) Tokenizacao
tks_disc <- corpus_disc %>%
tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(pattern = stopwords("pt")) %>%
tokens_remove(min_nchar = 5)
# 3) Matriz bag-of-words
tks_dfm <- dfm(tks_disc) %>%
dfm_trim(min_docfreq = 0.05, docfreq_type = "prop")
# 4) Transforma em tibble e inclui o target
dados <- as.matrix(tks_dfm) %>%
as_tibble() %>%
janitor::clean_names()
dados$y <- discursos$presidente
}
saco(discursos)
saco <- function(df, texto){
# 1) Criar um corpus
corpus_disc <- corpus(df, text_field = "texto")
# 2) Tokenizacao
tks_disc <- corpus_disc %>%
tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(pattern = stopwords("pt")) %>%
tokens_remove(min_nchar = 5)
# 3) Matriz bag-of-words
tks_dfm <- dfm(tks_disc) %>%
dfm_trim(min_docfreq = 0.05, docfreq_type = "prop")
# 4) Transforma em tibble e inclui o target
dados <- as.matrix(tks_dfm) %>%
as_tibble() %>%
janitor::clean_names()
dados$y <- discursos$presidente
}
saco(discursos, texto = discurso)
saco <- function(df, texto){
# 1) Criar um corpus
corpus_disc <- corpus(df, text_field = "texto")
# 2) Tokenizacao
tks_disc <- corpus_disc %>%
tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(pattern = stopwords("pt")) %>%
tokens_remove(min_nchar = 5)
# 3) Matriz bag-of-words
tks_dfm <- dfm(tks_disc) %>%
dfm_trim(min_docfreq = 0.05, docfreq_type = "prop")
# 4) Transforma em tibble e inclui o target
dados <- as.matrix(tks_dfm) %>%
as_tibble() %>%
janitor::clean_names()
dados$y <- discursos$presidente
}
saco(discursos, texto = "discurso")
saco <- function(df, texto){
# 1) Criar um corpus
corpus_disc <- corpus(df, text_field = texto)
# 2) Tokenizacao
tks_disc <- corpus_disc %>%
tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(pattern = stopwords("pt")) %>%
tokens_remove(min_nchar = 5)
# 3) Matriz bag-of-words
tks_dfm <- dfm(tks_disc) %>%
dfm_trim(min_docfreq = 0.05, docfreq_type = "prop")
# 4) Transforma em tibble e inclui o target
dados <- as.matrix(tks_dfm) %>%
as_tibble() %>%
janitor::clean_names()
dados$y <- discursos$presidente
}
saco(discursos, texto = "discurso")
saco <- function(df, texto){
# 1) Criar um corpus
corpus_disc <- corpus(df, text_field = texto)
# 2) Tokenizacao
tks_disc <- corpus_disc %>%
tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(pattern = stopwords("pt")) %>%
tokens_remove(min_nchar = 5)
# 3) Matriz bag-of-words
tks_dfm <- dfm(tks_disc) %>%
dfm_trim(min_docfreq = 0.05, docfreq_type = "prop")
# 4) Transforma em tibble e inclui o target
dados <- as.matrix(tks_dfm) %>%
as_tibble() %>%
janitor::clean_names()
dados$y <- discursos$presidente
}
sacopala <- saco(discursos, texto = "discurso")
?text_field
saco <- function(df, ntext){
# 1) Criar um corpus
corpus_disc <- corpus(df, text_field = ntext)
# 2) Tokenizacao
tks_disc <- corpus_disc %>%
tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(pattern = stopwords("pt")) %>%
tokens_remove(min_nchar = 5)
# 3) Matriz bag-of-words
tks_dfm <- dfm(tks_disc) %>%
dfm_trim(min_docfreq = 0.05, docfreq_type = "prop")
# 4) Transforma em tibble e inclui o target
dados <- as.matrix(tks_dfm) %>%
as_tibble() %>%
janitor::clean_names()
dados$y <- df$presidente
}
saco(discursos, 3)
saco <- function(df, ntext){
# 1) Criar um corpus
corpus_disc <- corpus(df, text_field = ntext)
# 2) Tokenizacao
tks_disc <- corpus_disc %>%
tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(pattern = stopwords("pt")) %>%
tokens_remove(min_nchar = 5)
# 3) Matriz bag-of-words
tks_dfm <- dfm(tks_disc) %>%
dfm_trim(min_docfreq = 0.05, docfreq_type = "prop")
# 4) Transforma em tibble e inclui o target
dados <- as.matrix(tks_dfm) %>%
as_tibble() %>%
janitor::clean_names()
dados$y <- df$presidente
}
saco(discursos, ntext = 3)
saco <- function(df, ntext){
# 1) Criar um corpus
corpus_disc <- corpus(df, text_field = ntext)
# 2) Tokenizacao
tks_disc <- corpus_disc %>%
tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(pattern = stopwords("pt")) %>%
tokens_remove(min_nchar = 5)
# 3) Matriz bag-of-words
tks_dfm <- dfm(tks_disc) %>%
dfm_trim(min_docfreq = 0.05, docfreq_type = "prop")
# 4) Transforma em tibble e inclui o target
dados <- as.matrix(tks_dfm) %>%
as_tibble() %>%
janitor::clean_names()
}
saco(discursos, ntext = 3)
saco <- function(df, ntext){
# 1) Criar um corpus
corpus_disc <- corpus(df, text_field = ntext)
# 2) Tokenizacao
tks_disc <- corpus_disc %>%
tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(pattern = stopwords("pt")) %>%
tokens_remove(min_nchar = 5)
# 3) Matriz bag-of-words
tks_dfm <- dfm(tks_disc) %>%
dfm_trim(min_docfreq = 0.05, docfreq_type = "prop")
# 4) Transforma em tibble e inclui o target
dados <- as.matrix(tks_dfm) %>%
as_tibble() %>%
janitor::clean_names()
}
sacopaia <- saco(discursos, ntext = 3)
View(sacopaia)
saco <- function(df){
# 1) Criar um corpus
corpus_disc <- corpus(df, text_field = "discurso")
# 2) Tokenizacao
tks_disc <- corpus_disc %>%
tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(pattern = stopwords("pt")) %>%
tokens_remove(min_nchar = 5)
# 3) Matriz bag-of-words
tks_dfm <- dfm(tks_disc) %>%
dfm_trim(min_docfreq = 0.05, docfreq_type = "prop")
# 4) Transforma em tibble e inclui o target
dados <- as.matrix(tks_dfm) %>%
as_tibble() %>%
janitor::clean_names()
dados$y <- discursos$presidente
}
saco(discursos)
saco <- function(df){
# 1) Criar um corpus
corpus_disc <- corpus(df, text_field = "discurso")
# 2) Tokenizacao
tks_disc <- corpus_disc %>%
tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(pattern = stopwords("pt")) %>%
tokens_remove(min_nchar = 5)
# 3) Matriz bag-of-words
tks_dfm <- dfm(tks_disc) %>%
dfm_trim(min_docfreq = 0.05, docfreq_type = "prop")
# 4) Transforma em tibble e inclui o target
dados <- as.matrix(tks_dfm) %>%
as_tibble() %>%
janitor::clean_names()
dados$y <- discursos$presidente
}
x <- saco(discursos)
saco <- function(df){
# 1) Criar um corpus
corpus_disc <- corpus(df, text_field = "discurso")
# 2) Tokenizacao
tks_disc <- corpus_disc %>%
tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(pattern = stopwords("pt")) %>%
tokens_remove(min_nchar = 5)
# 3) Matriz bag-of-words
tks_dfm <- dfm(tks_disc) %>%
dfm_trim(min_docfreq = 0.05, docfreq_type = "prop")
# 4) Transforma em tibble e inclui o target
dados <- as.matrix(tks_dfm) %>%
as_tibble() %>%
janitor::clean_names()
}
x <- saco(discursos)
link <- "https://github.com/FLS-6497/datasets/raw/main/aula5/discursos_presidenciais.csv"
discursos <- readr::read_csv2(link)
funcao_bow <- function(df, var){
# 1) Criar um corpus
corpus_disc <- corpus(df, text_field = var)
# 2) Tokenizacao
tks_disc <- corpus_disc %>%
tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(pattern = stopwords("pt")) %>%
tokens_remove(min_nchar = 5)
# 3) Matriz bag-of-words
tks_dfm <- dfm(tks_disc) %>%
dfm_trim(min_docfreq = 0.05, docfreq_type = "prop")
# 4) Transforma em tibble e inclui o target
dados <- as.matrix(tks_dfm) %>%
as_tibble() %>%
janitor::clean_names() %>%
mutate_all(as.numeric)
dados$y <- discursos$presidente
dados
}
x <- funcao_bow(discursos)
funcao_bow <- function(df, var){
# 1) Criar um corpus
corpus_disc <- corpus(df, text_field = var)
# 2) Tokenizacao
tks_disc <- corpus_disc %>%
tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(pattern = stopwords("pt")) %>%
tokens_remove(min_nchar = 5)
# 3) Matriz bag-of-words
tks_dfm <- dfm(tks_disc) %>%
dfm_trim(min_docfreq = 0.05, docfreq_type = "prop")
# 4) Transforma em tibble e inclui o target
dados <- as.matrix(tks_dfm) %>%
as_tibble() %>%
janitor::clean_names() %>%
mutate_all(as.numeric)
dados$y <- discursos$presidente
dados
}
x <- funcao_bow(discursos, var = "discurso")
View(x)
funcao_bow <- function(df, var){
# 1) Criar um corpus
corpus_disc <- corpus(df, text_field = var)
# 2) Tokenizacao
tks_disc <- corpus_disc %>%
tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(pattern = stopwords("pt")) %>%
tokens_remove(min_nchar = 5)
# 3) Matriz bag-of-words
tks_dfm <- dfm(tks_disc) %>%
dfm_trim(min_docfreq = 0.05, docfreq_type = "prop")
# 4) Transforma em tibble e inclui o target
dados <- as.matrix(tks_dfm) %>%
as_tibble() %>%
janitor::clean_names() %>%
mutate_all(as.numeric)
dados$y <- discursos$presidente
}
x <- funcao_bow(discursos, var = "discurso")
funcao_bow <- function(df, var){
# 1) Criar um corpus
corpus_disc <- corpus(df, text_field = var)
# 2) Tokenizacao
tks_disc <- corpus_disc %>%
tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(pattern = stopwords("pt")) %>%
tokens_remove(min_nchar = 5)
# 3) Matriz bag-of-words
tks_dfm <- dfm(tks_disc) %>%
dfm_trim(min_docfreq = 0.05, docfreq_type = "prop")
# 4) Transforma em tibble e inclui o target
dados <- as.matrix(tks_dfm) %>%
as_tibble() %>%
janitor::clean_names() %>%
mutate_all(as.numeric)
dados$y <- discursos$presidente
dados
}
x <- funcao_bow(discursos, var = "discurso")
?lrn
tsk <- as_task_classif(y ~ ., data = dados)
funcao_bow <- function(df, var){
# 1) Criar um corpus
corpus_disc <- corpus(df, text_field = var)
# 2) Tokenizacao
tks_disc <- corpus_disc %>%
tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(pattern = stopwords("pt")) %>%
tokens_remove(min_nchar = 5)
# 3) Matriz bag-of-words
tks_dfm <- dfm(tks_disc) %>%
dfm_trim(min_docfreq = 0.05, docfreq_type = "prop")
# 4) Transforma em tibble e inclui o target
dados <- as.matrix(tks_dfm) %>%
as_tibble() %>%
janitor::clean_names() %>%
mutate_all(as.numeric)
dados$y <- discursos$presidente
dados
}
dados <- funcao_bow(discursos, var = "discurso")
tsk <- as_task_classif(y ~ ., data = dados)
learner <- lrn("classif.naive_bayes")
resampling <- rsmp("holdout", ratio = 0.7)
resultados <- resample(tsk, learner, resampling)
resultados$score(msr("classif.f"))
tsk <- as_task_classif(y ~ ., data = dados)
learner <- lrn("classif.naive_bayes")
resampling <- rsmp("holdout", ratio = 0.7)
resultados <- resample(tsk, learner, resampling)
resultados$score(msr("classif.fbeta"))
tsk <- as_task_classif(y ~ ., data = dados)
learner <- lrn("classif.naive_bayes")
resampling <- rsmp("holdout", ratio = 0.7)
resultados <- resample(tsk, learner, resampling)
resultados$score(msr("classif.fbeta")) %>% pluck("classif.fbeta")
tsk <- as_task_classif(y ~ ., data = dados)
learner <- lrn("classif.rpart")
resampling <- rsmp("holdout", ratio = 0.7)
resultados <- resample(tsk, learner, resampling)
resultados$score(msr("classif.fbeta")) %>% pluck("classif.fbeta")
tsk <- as_task_classif(y ~ ., data = dados)
learner <- lrn("classif.kknn")
resampling <- rsmp("holdout", ratio = 0.7)
resultados <- resample(tsk, learner, resampling)
resultados$score(msr("classif.fbeta")) %>% pluck("classif.fbeta")
tsk <- as_task_classif(y ~ ., data = dados)
learner <- lrn("classif.ranger")
resampling <- rsmp("holdout", ratio = 0.7)
resultados <- resample(tsk, learner, resampling)
#Pacotes utilizados
if (!require("pacman")) install.packages("pacman"); # O Pacote "pacman" permite carregar os pacotes com menos cÃ³digo
# Carregando os pacotes
pacman::p_load("tidyverse",  "dplyr", "datasets", "ggplot2", "readxl", "haven", "knitr", "reshape2", "broom", "modelr", "stargazer", "jtools", "purrr", "mlr3", "mlr3measures", "mlr3viz", "mlr3learners", "GGally", "kknn", "glmnet", "quanteda", "janitor", "ranger")
#seed
set.seed(42)
tsk <- as_task_classif(y ~ ., data = dados)
learner <- lrn("classif.ranger")
resampling <- rsmp("holdout", ratio = 0.7)
resultados <- resample(tsk, learner, resampling)
resultados$score(msr("classif.fbeta")) %>% pluck("classif.fbeta")
?rmsp
??rsmp
funcao_bow <- function(df, var){
# 1) Criar um corpus
corpus_disc <- corpus(df, text_field = var)
# 2) Tokenizacao
tks_disc <- corpus_disc %>%
tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(pattern = stopwords("pt")) %>%
tokens_remove(min_nchar = 5)
# 3) Matriz bag-of-words
bow <- dfm(tks_disc) %>%
dfm_trim(min_docfreq = 0.05, docfreq_type = "prop")
# 4) Transforma em tibble e inclui o target
df <- bow %>%
as.matrix() %>%
as_tibble() %>%
janitor::clean_names() %>%
mutate_all(as.numeric)
df$y <- discursos$presidente
return(list(df=df, bow=bow))
}
amostra_treino$df
amostra_treino <- funcao_bow(discursos, var = "discurso")
amostra_treino$df
amostra_treino$bow
treino <- discursos %>%
sample_n(0.7 * length(discursos))
sample_n(0.7 * length("id")
discursos <- discursos %>%
discursos <- discursos %>%
mutate(id = row_number())
treino <- discursos %>%
sample_n(0.7 * length(id))
teste <- discursos %>%
filter(!id %in% treino$id)
treino <- discursos %>%
sample_n(0.9 * length(id))
teste <- discursos %>%
filter(!id %in% treino$id)
funcao_bow <- function(df, var){
# 1) Criar um corpus
corpus_disc <- corpus(df, text_field = var)
# 2) Tokenizacao
tks_disc <- corpus_disc %>%
tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(pattern = stopwords("pt")) %>%
tokens_remove(min_nchar = 5)
# 3) Matriz bag-of-words
bow <- dfm(tks_disc) %>%
dfm_trim(min_docfreq = 0.05, docfreq_type = "prop")
# 4) Transforma em tibble e inclui o target
df <- bow %>%
as.matrix() %>%
as_tibble() %>%
janitor::clean_names() %>%
mutate_all(as.numeric)
df$y <- df$presidente
return(list(df=df, bow=bow))
}
# Cria ids
discursos <- discursos %>%
mutate(id = row_number())
#Sorteia split-sample
treino <- discursos %>%
sample_n(0.9 * length(id))
teste <- discursos %>%
filter(!id %in% treino$id)
# BOW usando a base detreino
treino <- funcao_bow(treino, "discurso")
View(treino)
teste %>%
corpus(text_field="discurso") %>%
tokens() %>%
dfm() %>%
dfm_match(featnames(treino$bow))
# Naive bayes
tsk <- as_task_classif(y ~ ., data = treino$df)
learner <- lrn("classif.naive_bayes")
resultados$score(msr("classif.fbeta")) %>% pluck("classif.fbeta")
# Tree Learner
tsk <- as_task_classif(y ~ ., data = dados)
learner <- lrn("classif.rpart")
resultados$score(msr("classif.fbeta")) %>% pluck("classif.fbeta")
# Tree Learner
tsk <- as_task_classif(y ~ ., data = treino$df)
learner <- lrn("classif.rpart")
resultados$score(msr("classif.fbeta")) %>% pluck("classif.fbeta")
# K-nearest Neighbors
tsk <- as_task_classif(y ~ ., data = treino$df)
learner <- lrn("classif.kknn")
resampling <- rsmp("holdout", ratio = 0.7)
resultados <- resample(tsk, learner, resampling)
resultados$score(msr("classif.fbeta")) %>% pluck("classif.fbeta")
# K-nearest Neighbors
tsk <- as_task_classif(y ~ ., data = treino$df)
learner <- lrn("classif.kknn")
resultados$score(msr("classif.fbeta")) %>% pluck("classif.fbeta")
# Cria ids
discursos <- discursos %>%
mutate(id = row_number())
#Sorteia split-sample
treino <- discursos %>%
sample_n(0.9 * length(id))
teste <- discursos %>%
filter(!id %in% treino$id)
# BOW usando a base detreino
treino <- funcao_bow(treino, "discurso")
# Adequa a base de teste
teste_df <- teste %>%
corpus(text_field="discurso") %>%
tokens() %>%
dfm() %>%
dfm_match(featnames(treino$bow)) %>%
as.matrix() %>%
as_tibble() %>%
janitor::clean_names()
teste_df$y <- teste$presidente
View(teste_df)
